"""
Simple Demo Ğ±ĞµĞ· vLLM - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· transformers
"""
import os
# ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ: Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ”Ğ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ° PyTorch
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸

import gradio as gr
import torch
from PIL import Image
from pathlib import Path

print(f"âœ“ PYTORCH_CUDA_ALLOC_CONF={os.environ['PYTORCH_CUDA_ALLOC_CONF']}")

# Ğ’ĞĞ–ĞĞ: ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ GPU Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ÑÑ€Ğ°Ğ·Ñƒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ° torch
if torch.cuda.is_available():
    # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾ 80% (6.4GB Ğ¸Ğ· 8GB) - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ~5.5GB
    torch.cuda.set_per_process_memory_fraction(0.8, 0)
    # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ ĞºĞµÑˆ Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    torch.cuda.empty_cache()
    print(f"âœ“ GPU memory limit set to 80% (~6.4GB)")
    print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
    print(f"âœ“ Total GPU memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB")

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹
from dots_ocr.parser import DotsOCRParser
from dots_ocr.utils import dict_promptmode_to_prompt

print("Loading model...")
print(f"â³ GPU memory before loading: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB allocated")
model_path = "./weights/DotsOCR"

# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€ÑĞµÑ€ (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ HuggingFace Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ğ±ĞµĞ· vLLM)
parser = DotsOCRParser(
    use_hf=True  # ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€!
)

print(f"âœ“ GPU memory after loading: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB allocated")
print(f"âœ“ GPU memory reserved: {torch.cuda.memory_reserved(0)/1024**3:.2f}GB")

print(f"âœ“ Model loaded (HuggingFace mode)")
print(f"âœ“ CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
    print(f"âœ“ Total GPU memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB")
    print(f"âœ“ Memory limit: {torch.cuda.get_device_properties(0).total_memory*0.8/1024**3:.2f}GB (80%)")
    print(f"âœ“ Current allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB")
    print(f"âœ“ Current reserved: {torch.cuda.memory_reserved(0)/1024**3:.2f}GB")

def process_image(image, prompt_mode):
    """Process image with DotsOCR"""
    if image is None:
        return None, "Please upload an image", "", ""
    
    try:
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
        prompt = dict_promptmode_to_prompt.get(prompt_mode, dict_promptmode_to_prompt["prompt_layout_all_en"])
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ temp ĞµÑĞ»Ğ¸ ĞµÑ‘ Ğ½ĞµÑ‚
        import os
        os.makedirs("./temp", exist_ok=True)
        
        # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        import time
        start_time = time.time()
        
        # GPU Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³
        if torch.cuda.is_available():
            gpu_mem_before = torch.cuda.memory_allocated(0) / 1024**3  # GB
            gpu_mem_reserved_before = torch.cuda.memory_reserved(0) / 1024**3  # GB
        
        print(f"\n{'='*60}")
        print(f"ğŸ” ĞĞĞ§ĞĞ›Ğ ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ˜")
        print(f"{'='*60}")
        print(f"ğŸ“„ Ğ¢Ğ¸Ğ¿ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {type(image)}")
        print(f"ğŸ“ Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {image.size if hasattr(image, 'size') else 'Unknown'}")
        print(f"ğŸ¯ Prompt mode: {prompt_mode}")
        print(f"ğŸ“ Prompt: {prompt[:100]}...")
        if torch.cuda.is_available():
            print(f"ğŸ® GPU Memory (before): {gpu_mem_before:.2f}GB allocated, {gpu_mem_reserved_before:.2f}GB reserved")
        print(f"{'='*60}\n")
        
        # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
        result = parser.parse_image(
            input_path=image,
            filename="demo",
            prompt_mode=prompt_mode,
            save_dir="./temp"
        )
        
        processing_time = time.time() - start_time
        
        # GPU Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        if torch.cuda.is_available():
            gpu_mem_after = torch.cuda.memory_allocated(0) / 1024**3  # GB
            gpu_mem_reserved_after = torch.cuda.memory_reserved(0) / 1024**3  # GB
            gpu_mem_used = gpu_mem_after - gpu_mem_before
        
        print(f"\n{'='*60}")
        print(f"âœ… Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ˜")
        print(f"{'='*60}")
        print(f"â±ï¸  Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {processing_time:.2f}s")
        if torch.cuda.is_available():
            print(f"ğŸ® GPU Memory (after): {gpu_mem_after:.2f}GB allocated, {gpu_mem_reserved_after:.2f}GB reserved")
            print(f"ğŸ“Š GPU Memory used: {gpu_mem_used:+.2f}GB")
            print(f"ğŸ“ˆ Peak memory: {torch.cuda.max_memory_allocated(0)/1024**3:.2f}GB")
        print(f"ğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: {len(result) if result else 0}")
        
        if result and len(result) > 0:
            result_data = result[0]
            
            print(f"ğŸ”‘ ĞšĞ»ÑÑ‡Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°: {list(result_data.keys())}")
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
            md_content = result_data.get('md_content', '')
            layout_image = result_data.get('layout_image', image)
            cells_data = result_data.get('cells_data', [])
            
            print(f"ğŸ“ Markdown Ğ´Ğ»Ğ¸Ğ½Ğ°: {len(md_content) if md_content else 0} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")
            print(f"ğŸ”¢ ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: {len(cells_data) if cells_data else 0}")
            print(f"{'='*60}\n")
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°
            process_log = f"""### ğŸ” ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸:

**1. Ğ’Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ:**
- Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {image.size if hasattr(image, 'size') else 'Unknown'}
- Prompt mode: `{prompt_mode}`
- Prompt: `{prompt[:80]}...`

**2. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°:**
- Ğ’Ñ€ĞµĞ¼Ñ: {processing_time:.2f}s
- ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: {len(cells_data) if cells_data else 0}

**3. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:**
- Markdown: {len(md_content) if md_content else 0} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²
- Layout image: {'âœ… Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½' if layout_image else 'âŒ ĞĞµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½'}
"""
            
            info = f"""### âœ… Processing Complete!
**Prompt Mode:** {prompt_mode}
**Status:** Success
**Time:** {processing_time:.2f}s
**Elements detected:** {len(cells_data) if cells_data else 0}
**Mode:** HuggingFace (No vLLM)
"""
            
            if not md_content:
                md_content = "âš ï¸ Markdown content is empty. Check process log."
            
            return layout_image, info, md_content, process_log
        else:
            print(f"âš ï¸  ĞĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²!")
            print(f"{'='*60}\n")
            return image, "âš ï¸  No results returned", "", "âŒ No results from parser"
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        
        print(f"\n{'='*60}")
        print(f"âŒ ĞĞ¨Ğ˜Ğ‘ĞšĞ:")
        print(f"{'='*60}")
        print(error_trace)
        print(f"{'='*60}\n")
        
        error_msg = f"""### âŒ Error occurred:
```
{str(e)}
```
"""
        error_log = f"""### Traceback:
```
{error_trace}
```
"""
        return image, error_msg, "", error_log

# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Gradio Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ
with gr.Blocks(theme="ocean", title="dots.ocr Simple Demo") as demo:
    gr.HTML("""
        <div style="text-align: center; margin-bottom: 20px;">
            <h1>ğŸ” dots.ocr Simple Demo</h1>
            <p><em>Document OCR with Layout Analysis (No vLLM required)</em></p>
        </div>
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“¥ Input")
            image_input = gr.Image(type="pil", label="Upload Image")
            
            prompt_mode = gr.Dropdown(
                label="Prompt Mode",
                choices=list(dict_promptmode_to_prompt.keys()),
                value="prompt_layout_all_en"
            )
            
            process_btn = gr.Button("ğŸ” Process", variant="primary")
            clear_btn = gr.Button("ğŸ—‘ï¸ Clear", variant="secondary")
            
            info_display = gr.Markdown("Waiting for input...")
        
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ‘ï¸  Layout Result")
            layout_output = gr.Image(label="Layout Analysis")
            
        with gr.Column(scale=1):
            gr.Markdown("### âœ”ï¸ Results")
            
            with gr.Tabs():
                with gr.TabItem("ğŸ“ Markdown Output"):
                    md_output = gr.Markdown("Waiting for processing...")
                
                with gr.TabItem("ğŸ” Process Log"):
                    process_log = gr.Markdown("Waiting for processing...")
    
    # Event handlers
    process_btn.click(
        fn=process_image,
        inputs=[image_input, prompt_mode],
        outputs=[layout_output, info_display, md_output, process_log]
    )
    
    clear_btn.click(
        fn=lambda: (None, "Waiting for input...", "Waiting for processing...", "Waiting for processing..."),
        inputs=[],
        outputs=[layout_output, info_display, md_output, process_log]
    )

if __name__ == "__main__":
    import sys
    port = int(sys.argv[1]) if len(sys.argv) > 1 else 7860
    
    print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"ğŸš€ Starting Gradio on 0.0.0.0:{port}")
    print(f"   Accessible from: http://192.168.1.115:{port}")
    print(f"   Or any local IP on port {port}")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=port,
        share=False
    )

